{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# append root project directory so packages will be available\n",
    "sys.path.append('../..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import src.utility.symbols_loader as sl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import einops\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from src.architectures.course_autoencoder import Autoencoder, Encoder, Decoder\n",
    "from src.architectures.letters_dataset import LettersDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True # should improve speed if input size don't change"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "LATENT_WIDTH = 16\n",
    "FILE_PATH = Path().parent.as_posix() + \"/\"\n",
    "path = FILE_PATH + \"../../data/models/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/405 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trials = ['trial_0', 'trial_1', 'trial_2', 'trial_3', 'trial_4']\n",
    "\n",
    "for trial in trials:\n",
    "    emnist_Autoencoder = Autoencoder(Encoder(LATENT_WIDTH), Decoder(LATENT_WIDTH))\n",
    "    emnist_Autoencoder.load_state_dict(torch.load(path + trial +'_emnist_autoencoder.pth'))\n",
    "    emnist_Encoder = emnist_Autoencoder.encoder\n",
    "    emnist_Encoder.to(device)\n",
    "    mnist_dataset = LettersDataset(sl.load_emnist_pages(number_of_pages=5, trial=trial))\n",
    "\n",
    "    emnist_val_loader = DataLoader(\n",
    "        mnist_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    loss_func = F.l1_loss\n",
    "    optimizer = torch.optim.Adam(emnist_Autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "    emnist_preds = []\n",
    "\n",
    "    with torch.no_grad() as nograd:\n",
    "        for batch in tqdm(emnist_val_loader):\n",
    "\n",
    "            images = batch.to(device)\n",
    "            predictions = emnist_Encoder(images)\n",
    "\n",
    "            emnist_preds.append(predictions.cpu())\n",
    "\n",
    "    emnist_preds_numpy = np.zeros((len(emnist_preds) * 32, LATENT_WIDTH))\n",
    "\n",
    "    iterator = 0\n",
    "    for tensor in emnist_preds:\n",
    "        for block in range(tensor.shape[0]):\n",
    "            emnist_preds_numpy[iterator] = tensor[block].numpy().copy()\n",
    "            iterator += 1\n",
    "\n",
    "    path = FILE_PATH + \"../../data/encoded_data/\" + trial + '/'\n",
    "    np.savez_compressed(path + \"emnist_preds.npz\", emnist_preds_numpy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trials = ['trial_0', 'trial_1', 'trial_2', 'trial_3', 'trial_4']\n",
    "\n",
    "for trial in trials:\n",
    "    kuzushiji_Autoencoder = Autoencoder(Encoder(LATENT_WIDTH), Decoder(LATENT_WIDTH))\n",
    "    kuzushiji_Autoencoder.load_state_dict(torch.load(path + trial +'/_kuzushiji_autoencoder.pth'))\n",
    "    kuzushiji_Encoder = kuzushiji_Autoencoder.encoder\n",
    "    kuzushiji_Encoder.to(device)\n",
    "    kuzushiji_dataset = LettersDataset(sl.load_kuzushiji_pages(number_of_pages=5, trial=trial))\n",
    "\n",
    "    kuzushiji_val_loader = DataLoader(\n",
    "        kuzushiji_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    loss_func = F.l1_loss\n",
    "    optimizer = torch.optim.Adam(kuzushiji_Autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "    kuzushiji_preds = []\n",
    "\n",
    "    with torch.no_grad() as nograd:\n",
    "        for batch in tqdm(kuzushiji_val_loader):\n",
    "\n",
    "            images = batch.to(device)\n",
    "            predictions = kuzushiji_Encoder(images)\n",
    "\n",
    "            kuzushiji_preds.append(predictions.cpu())\n",
    "\n",
    "    kuzushiji_preds_numpy = np.zeros((len(kuzushiji_preds) * 32, LATENT_WIDTH))\n",
    "\n",
    "    iterator = 0\n",
    "    for tensor in kuzushiji_preds:\n",
    "        for block in range(tensor.shape[0]):\n",
    "            kuzushiji_preds_numpy[iterator] = tensor[block].numpy().copy()\n",
    "            iterator += 1\n",
    "\n",
    "    path = FILE_PATH + \"../../data/encoded_data/\" + trial + '/'\n",
    "    np.savez_compressed(path + \"kuzushiji_preds.npz\", kuzushiji_preds_numpy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}